# Probabilistic Graphical Model

## 1 Basic Concepts

在概率图模型中，每个结点表示⼀个随机变量（或⼀组随机变量），链接表示这些变量之间的概率关系。这样，**图描述了联合概率分布在所有随机变量上能够分解为⼀组因子的乘积的方式**，每个因子只依赖于随机变量的⼀个子集。

传统上，从概率图的表示形式上，可以将其分为概率有向图模型，即*贝叶斯网络*（Bayesian network），和概率无向图模型，即*马尔科夫随机场*（Markov random fields）。

## 2 Bayesian Network

### 2.1 Concepts of Probabilistic Directed Graphs

贝叶斯网络以有向图的形式描述联合概率的分解方式，例如，对于一个一般的联合概率分布 $p(a,b,c)$，可以将其分解为：
$$
p(a,b,c)=p(a)p(b\mid a)p(c\mid a,b)
$$

然后，我们为每个变量分配一个节点，并以有向边的形式描述变量之间的依赖关系，这就形成了有向概率图：
![p(abc)](./images/9.1.png)

关于上述分解方式，有趣的一点是左侧是对称的，而右侧是非对称的，这就说明我们为这三个变量指定了一个隐含的顺序。

对于一个更一般的有 $K$ 个节点的有向概率图，其表示的联合概率分布可以写作：

$$
p(\mathbf{x})=\prod_{k=1}^Kp(x_k\mid \mathrm{pa}_k)
$$

其中 $\mathrm{pa}_k$ 表示节点 $x_k$ 的父节点的集合。

贝叶斯网络的图结构需要满足的一个性质是无环，即所有贝叶斯网络均是有向无环图（DAG）。

### 2.2 Patterns of Subgraphs

贝叶斯网络中有三种基本的子图模式：

#### 2.2.1 Tail-to-Tail

![T-to-T](./images/9.2.png)

根据有向图的联合概率分解方式，该概率图表示的联合概率可以写作：
$$
p(a,b,c)=p(a\mid c)p(b\mid c)p(c)
$$

那么，根据上式，我们可以写出：
$$
p(a,b)=\int_cp(a,b,c)\mathrm{d}c=\int_cp(a\mid c)p(b\mid c)p(c)\mathrm{d}c\not ={p(a)p(b)}
$$

$$
p(a,b\mid c)=\frac{p(a,b,c)}{p(c)}=p(a\mid c)p(b\mid c)
$$

这说明，通常情况下变量 $a$ 与 $b$ 并不独立。而给定变量 $c$ 的条件下变量 $a$ 与 $b$ 条件独立。

#### 2.2.2 Tail-to-Head

![T-to-H](./images/9.3.png)

对于这种情况，可以类似地对联合概率进行分解：
$$
p(a,b,c)=p(a)p(c\mid a)p(b\mid c)
$$

同样地，根据上式写出：
$$
p(a,b)=\int_cp(a,b,c)\mathrm{d}c=p(a)\int_cp(c\mid a)p(b\mid c)\mathrm{d}c=p(a)p(b\mid a)\not ={p(a)p(b)}
$$

$$
p(a,b\mid c)=\frac{p(a,b,c)}{p(c)}=\frac{p(a)p(c\mid a)p(b\mid c)}{p(c)}=\frac{p(a,c)}{p(c)}p(b\mid c)=p(a\mid c)p(b\mid c)
$$

这显示该子图的性质与第一种子图模式类似，即在通常情况下变量 $a$ 与 $b$ 并不独立。而给定变量 $c$ 的条件下变量 $a$ 与 $b$ 条件独立。

#### 2.2.3 Head-to-Head

![H-to-H](./images/9.4.png)

进行类似的联合概率分解处理：
$$
p(a,b,c)=p(a)p(b)p(c\mid a,b)
$$

这种子图模式与前两种存在显著的区别：
$$
p(a,b)=\int_cp(a,b,c)\mathrm{d}c=p(a)p(b)\int_cp(c\mid a,b)\mathrm{d}c=p(a)p(b)
$$

这表明在通常情况下变量 $a$ 和变量 $b$ 就是独立的。
$$
p(a,b\mid c)=\frac{p(a,b,c)}{p(c)}=\frac{p(a)p(b)p(c\mid a,b)}{p(c)}\not ={p(a\mid c)p(b\mid c)}
$$

这表明在以观测到的 $c$ 值作为条件时，变量 $a$ 和变量 $b$ 之间反而不独立了。

更一般的结论是，在这类的子图中，如果 $c$ 节点或者它的任何一个后继被观测到，那么变量 $a$ 和变量 $b$ 之间就不再独立，或者说路径“解除阻隔”。

### 2.3 d-Separation

d-separation 刻画了变量之间的条件独立关系与其概率图的拓扑结构的等价关系，提供了一种更通用的从有向概率图中提取条件独立性的框架。d-separation 将有向图的节点划分为三个节点集合 $A$、$B$ 和 $C$，当以下条件均被满足时，认为 $A$ 中节点和 $B$ 中节点在给定 $C$ 中节点时条件独立：

- $A$ 中任意节点到 $B$ 中任意节点的路径中，head-to-tail 和 tail-to-tail 型的节点至少有一个位于集合 $C$ 中
- $A$ 中任意节点到 $B$ 中任意节点的路径中，head-to-head 型的节点及其后继至少有一个均不位于集合 $C$ 中

我们称 $C$ 是对 $A$、$B$ 的 d-separation。

### 2.4 Markov Blanket

对于一个含有 $D$ 个节点的概率图，根据联合概率的因子分解性质，我们可以将某个节点的条件概率写作：
$$
p(x_i\mid x_{\{j\not ={i}\}})=\frac{p(x_1,x_2,\dotsb,x_D)}{\int_{x_i}p(x_1,x_2,\dotsb,x_D)\mathrm{d}x_i}=\frac{\prod_kp(x_k\mid \mathrm{pa}_k)}{\int_{x_i}\prod_kp(x_k\mid \mathrm{pa}_k)\mathrm{d}x_i}
$$

观察上式，形如 $p(x_i\mid \mathrm{pa}_i)$ 的因式依赖于节点 $x_i$ 的父节点，而形如 $p(x_j\mid \mathrm{pa}_j),i\not ={j},x_i\in\mathrm{pa}_j$ 的因式依赖于 $x_i$ 的子节点及配偶节点，我们把由某个节点的父节点、子节点、配偶节点组成的节点集合称为该节点的马尔可夫毯，一个节点的条件概率分布独立于除其马尔可夫毯中节点外的所有节点。

![d-separation](./images/9.5.png)

## 3 Markov Random Fields

马尔可夫随机场是无向的概率图模型，在贝叶斯网络中，我们首先讨论了联合概率的分解规则，然后讨论了与之对应的条件独立性（d-separation），在无向图模型中，首先对条件独立性进行讨论更加方便。

### 3.1 Conditional Independence

类似于 d-separation 我们考虑三个节点集合，$A$、$B$ 和 $C$。由于马尔可夫随机场中边的无向性，head-to-head 型节点的特殊性不能体现，因此，无向图中只满足对于连接 $A$、$B$ 中任意节点的任意路径，其途径的节点均位于 $C$ 中，且不存在连接二者的直接路径，即可认为 $A$ 中节点和 $B$ 中节点在给定 $C$ 中节点时条件独立。

由此，我们可以推出无向图模型中的 Markov blanket 只包括相邻节点，即节点的条件概率只依赖于相邻节点，与其他节点无关。

![Markov blanket](./images/9.6.png)

### 3.2 Decomposition Property

通过无向图的条件独立性，我们可以推出，对于图中任意两个没有直接相连的节点 $x_i$、$x_j$，在给定其他节点的情况下，这两个节点一定是条件独立的，因为连接这两个节点的所有路径均被观测节点阻隔：
$$
p(x_i,x_j\mid \mathbf{x}_{\backslash\{i,j\}})=p(x_i\mid \mathbf{x}_{\backslash\{i,j\}})p(x_j\mid \mathbf{x}_{\backslash\{i,j\}})
$$

因此，联合概率的分解规则需要满足不使 $x_i$、$x_j$ 出现在同一因子中，从而使属于这个图的所有可能的概率分布均满足条件独立性质。由此我们使用*团块*（clique）的概念，团块是图中节点的一个子集，且集合中任意两个节点之间均存在直接连接，*最大团块*（maximal clique）是指无法新纳入任何一个图中节点使得团块仍然符合定义要求。于是，我们可以将联合概率分布分解的因子定义为（最大）团块的函数，将团块记作 $C$，将团块中的元素记作 $\mathbf{x}_C$，联合概率分布可以写作图的最大团块的势函数 $\psi_C(\mathbf{x}_C)$ 的乘积的（归一化）形式：
$$
p(\mathbf{x})=\frac{1}{Z}\prod_C\psi_C(\mathbf{x}_C)
$$

其中，$Z$ 为划分函数（partition function），$Z=\sum_\mathbf{x}\prod_C\psi_C(\mathbf{x}_C)$，其意义是保证联合概率分布被正确地归一化，划分函数的存在主要是由于在无向图模型中我们并不对势函数的形式进行限制，并不假定其具有具体的概率意义。划分函数的计算是无向图的一个主要缺点其计算复杂度为 $O(m^n)$，$m$ 为节点状态数，$n$ 为图中的节点数。对于局部条件概率的计算，归一化常数是不需要的。

由于势函数被限制为严格大于零，因此使用指数形式的势函数是合理的：
$$
\psi_C(\mathbf{x}_C)=\exp\{-E(\mathbf{x}_C)\}
$$

其中，$E(\mathbf{x}_C)$ 称为能量函数，该指数表示被称为*玻尔兹曼分布*（Boltzmann distribution），联合概率被定义为势函数的乘积，因此总能量可以通过将每个最大团的能量相加得到。

由于势函数并不具有概率意义，因此选择势函数的自由度较大，但是这也使得选择合适的势函数成为了一个问题。

Hammersley-Clifford 定理保证了划分性质和条件独立性质对概率分布的约束是等价的。

### 3.3 Relationship to Directed Graphs

考虑如何将一个有向概率图转化为无向概率图，只需要找到两者联合概率因子分解之间的关系，然后将无向概率图中的势函数以有向概率图中联合概率的因子分解中的条件概率因式表示即可，首先是一种最简单的情况：
![relation](./images/9.7.png)

那么，在有向图中的因子分解可以写作：
$$
p(\mathbf{x})=p(x_1)p(x_2\mid x_1)\dotsb p(x_{N}\mid x_{N-1})
$$

而无向图表示中，最大团为两个相邻的节点，其因子分解应当满足以下的形式：
$$
p(\mathbf{x})=\frac{1}{Z}\psi_{1,2}(x_1,x_2)\psi_{2,3}(x_2,x_3)\dotsb\psi_{N-1,N}(x_{N-1},x_N)
$$

那么我们么只需要简单地令：
$$
\psi_{1,2}(x_1,x_2)=p(x_1)p(x_2\mid x_1)
$$

$$
\psi_{1,2}(x_2,x_3)=p(x_3\mid x_2)
$$

$$
\dotsb
$$

$$
\psi_{1,2}(x_N,x_{N-1})=p(x_N\mid x_{N-1})
$$

同时令 $Z=1$ 即可满足要求。

对于更一般的有向图，我们需要确保因子分解中的每个条件概率因式至少出现在一个势函数中，即每个因式中的节点至少属于同一个最大团，对于父节点单一的节点，这是易于保证的，可以简单地将有向边直接替换为无向边，但是对于 head-to-head 型的节点，由于其条件概率因式同时依赖于所有父节点，而这些父节点往往并不能直接属于同一个最大团，因此，我们需要在这些父节点之间建立新的连接，确保它们是全连通的。
![moral graph](./images/9.8.png)

以此方式形成的无向概率图称为道德图（moral graph）。将有向图转换为无向图之后，图中的节点失去了部分条件独立性质。

### 3.4 Denpendency Map、Independence Map、Perfect Map

如果一个概率分布中的所有条件独立性质都通过一个图反映出来，那么这个图被称为这个概率分布的 D 图（dependency map），因此，一个完全非连接的图是任何概率分布的平凡 D 图，因为其反映所有形式的条件独立性质。

如果一个图中的每个条件独立性质都由某个概率分布满足，那么称这个图这这个概率分布的 I 图（independence map），因此，一个全连接的图是任何概率分布的平凡 I 图，因为其不反应任何形式的条件独立性质。

如果概率分布的每个条件独立性质都由可以由图反映出来，反之也成立，那么这个图被称为是概率分布的完美图（perfect map），一个完美图既是 I 图又是 D 图。

## 4 Inference

将图中的一些节点限制为观测值，尝试求出其他节点的后验概率分布，这类问题称为推断（inference）问题。概率图的推断方式可以分为精确推断、近似推断两种。

### 4.1 Inference for Chain Structure

对于一个链结构的有向图，我们可以将其转化为等价的无向图，并给出其联合概率的因子分解形式：
$$
p(\mathbf{x})=\frac{1}{Z}\psi_{1,2}(x_1,x_2)\psi_{2,3}(x_2,x_3)\dotsb\psi_{N-1,N}(x_{N-1},x_N)
$$

为了解决求边缘概率分布 $p(x_n)$ 的推断问题，我们可以通过求和或积分的形式在联合概率中消去其余变量：
$$
p(x_n)=\sum_{x_1}\sum_{x_2}\dotsb\sum_{x_{n-1}}\sum_{x_n}\dotsb\sum_{x_N}\frac{1}{Z}\psi_{1,2}(x_1,x_2)\psi_{2,3}(x_2,x_3)\dotsb\psi_{N-1,N}(x_{N-1},x_N)
$$

这种计算方式称为变量消除，以这种方式直接进行计算，计算复杂度为 $O(K^N)$，$K$ 为节点状态数，$N$ 为节点数，因此计算代价是极大的。

考察上式对各节点状态的依赖形式，我们可以写出以下变形：
$$
p(x_n)=\frac{1}{Z}\left[\sum_{x_{n-1}}\psi_{n-1,n}(x_{n-1},x_{n})\left[\sum_{x_{n-2}}\psi_{n-2,n-1}(x_{n-3},x_{n-2})\dotsb\left[\sum_{x_1}\psi_{1,2}(x_1,x_2)\right]\dotsb\right]\right]\left[\sum_{x_{n+1}}\psi_{n,n+1}(x_{n},x_{n+1})\dotsb\left[\sum_{x_{N}}\psi_{N-1,N}(x_{N-1},x_{N})\right]\dotsb\right]
$$

这种简化形式的基本原理是乘法分配律：
$$
ab+ac=a(b+c)
$$

使用这种形式进行计算，计算复杂度为 $O(NK^2)$。

直接使用上式进行计算的问题是，如果需要对多个节点的边缘概率分布进行计算，需要计算大量重复的因式，计算效率低。观察上式，可见，一个节点的边缘概率分布可以被写作两个部分，我们给出以下定义：
$$
\mu_\alpha(x_n)=\sum_{x_{n-1}}\psi_{n-1,n}(x_{n-1},x_{n})\left[\sum_{x_{n-2}}\dotsb\right]=\sum_{x_{n-1}}\psi_{n-1,n}(x_{n-1},x_{n})\mu_\alpha(x_{n-1})
$$

$$
\mu_\beta(x_n)=\sum_{x_{n+1}}\psi_{n,n+1}(x_{n},x_{n+1})\left[\sum_{x_{n+2}}\dotsb\right]=\sum_{x_{n+1}}\psi_{n,n+1}(x_{n},x_{n+1})\mu_\beta(x_{n+1})
$$

那么边缘概率分布可以表示为：
$$
p(x_n)=\frac{1}{Z}\mu_\alpha(x_n)\mu_\beta(x_n)
$$

这种推断方式将边缘概率分布表示为了前后向的信念的乘积。

特别地，如果有变量的值被观测到，那么其值只需要限制为观测值即可，无需求和处理。

对于两个相邻节点的联合概率分布，求法是类似的：
$$
p(x_{n-1},x_n)=\frac{1}{Z}\mu_\alpha(x_{n-1})\psi_{n-1,n}(x_{n-1},x_{n})\mu_\beta(x_n)
$$

### 4.2 Inference for Tree Structure

树是一种特殊的图结构：

- 在无向图中，树是指任意一对节点之间只有一条路径的图
- 在有向图中，树是指仅有一个节点无父节点（根节点），其余节点均只有一个父节点的图

因此，一个有向树对应的道德图不会增加任何边，同时是无环的。

如果有向图不满足父节点数量的限制，但是任意两个节点之间只有一条路径（忽略边的方向），这样的图被称为多树（polytree）。

#### 4.2.1 Factor-Graph

因子图将联合概率分布的因子分解方式纳入了图的显式表示中，如果将联合概率分布表示为一组因子的乘积：
$$
p(x)=\prod_sf_s(\mathbf{x_s})
$$

$\mathbf{x_s}$ 表示与因子相关的变量的集合，这个表述对于有向图和无向图是统一的。在因子图中，除了使用节点表示变量之外，还使用另一组节点表示因子本身，例如对一个因子分解为 $p(\mathbf{x})=f_a(x_1,x_2)f_b(x_1,x_2)f_c(x_2,x_3)f_d(x_3)$ 的概率分布对应的因子图为：
![factor graph](./images/9.9.png)

因子图是二分图，因为其是异质图，含有两类节点，同时所有边均位于两类节点之间。

一个图可以对应多个等价的因子图形式，将一个有向树或无向树转化为因子图，生成的因子图也是树。

#### 4.2.2 Sum-Product Algorithm

加和-乘积算法是一种精确推断算法，在无环图中，其相当于信念传播（belief propagation）算法。

首先将图转化为因子图，使得我们可以统一地处理有向图和无向图，根据定义，边缘概率分布通过对所有 $x$ 之外的变量上的联合概率分布进行求和的方式得到：
$$
p(x)=\sum_{\mathbf{x}\backslash x}p(\mathbf{x})
$$

对于有树结构的图，我们可以将联合概率中的因子划分为若干组：
$$
p(\mathbf{x})=\prod_{s\in\mathrm{ne}(x)}F_s(x,X_s)
$$

$\mathrm{ne}(x)$ 表示与变量节点 $x$ 相连的因子节点集合，$X_s$ 表示所有通过因子节点 $f_s$ 与变量节点 $x$ 相关联的变量节点，$F_s(x,X_s)$ 表示分式中所有与 $f_s$ 相关联的因式的乘积：

![Sum-Product Algorithm](./images/9.10.png)

带入边缘概率的表达式，同时类似与链结构中的操作，交换加法与乘积的位置，得到：
$$
p(x)=\sum_{\mathbf{x}\backslash x}\prod_{s\in\mathrm{ne}(x)}F_s(x,X_s)=\prod_{s\in\mathrm{ne}(x)}\left[\sum_{X_s}F_s(x,X_s)\right]=\prod_{s\in \mathrm{ne}(x)}\mu_{f_s\rightarrow x}(x)
$$

称 $\mu_{f_s\rightarrow x}(x)$ 为因子节点 $f_s$ 传递至变量节点 $x$ 的信息（message）。

注意到，每个因子 $F_s(x,X_s)$ 均可以表示为因子子图，因此本身可以被单独分解：
$$
\begin{aligned}
F_s(x,X_s)&=f_s(x,x_1,\dotsb,x_M)G_1(x_1,X_{s1})\dotsb G_M(x_2,X_{s2})\\
&=f_s(x,x_1,\dotsb,x_M)\prod_{m\in\mathrm{ne}(f_s)\backslash x}\left[\sum_{X_{sm}}G_m(x_m,X_{sm})\right]\\
&=f_s(x,x_1,\dotsb,x_M)\prod_{m\in\mathrm{ne}(f_s)\backslash x}\mu_{x_m\rightarrow f_s}(x_m)\\
\end{aligned}
$$
式中，$G_m$ 是对与因子节点 $f_s$ 相连的、不依赖变量节点 $x$ 的各子图的分解，将 $\sum_{X_{sm}}G_m(x_m,X_{sm})$ 定义为由变量节点 $x_m$ 传递至因子节点 $f_s$ 的信息。

![Sum-Product Algorithm](..\images\9.11.png)

因此，一个变量节点通过一个链接发送到一个因子节点的信息可以通过计算沿着所有进⼊因⼦结点的其他链接的输⼊信息的乘积，乘以与那个结点关联的因⼦，然后对所有与输⼊信息关联的变量进行求和，**一旦一个因子节点从所有其他相邻变量节点接收信息，那么这个因子就可以向变量节点发送信息**。

同时由于：
$$
G_m(x_s,X_m)=\prod_{l\in\mathrm{ne}(x_m)\backslash f_s}F_l(x_m,X_m)
$$
因此：
$$
\mu_{x_m\rightarrow f_s(x_m)}=\prod_{l\in\mathrm{ne}(x_m)\backslash f_s}\left[\sum_{X_{lm}}F_l(x_m,X_m)\right]=\prod_{l\in\mathrm{ne}(x_m)\backslash f_s}\mu_{f_l\rightarrow x_m(x_m)}
$$
如果将节点 $x$ 看作树的根节点，然后从叶节点开始计算，那么，如果一个变量节点是一个叶节点，则它沿着与它相连的唯一路径传递的信息是：
$$
\mu_{x\rightarrow f}(x)=1
$$
如果因子节点是一个叶节点，其发送的信息是：
$$
\mu_{f\rightarrow x}(x)=f(x)
$$
那么，假设我们需要对图中的每一个节点计算边缘概率分布，可以通过简单地分别将每个节点视作根节点并执行以上过程完成，但是这样做极其繁琐，合理的做法是使用一般的加和-乘积算法，首先任意指定一个变量节点将其视为根节点，然后由叶节点向根节点传递信息，此时，图中的节点可以分为以下两种情况：

- **根节点：**接受所有相邻节点的信息，可以向所有方向发送信息
- **其他节点：**接受除根节点方向外所有节点的信息，在接收到根节点方向的信息后可以向远离根节点的所有方向发送信息

由此，我们可以进行上述过程的反向过程，使得所有节点既向所有相邻节点发送过信息，又接受过所有相邻节点发送的信息，这使得图中所有节点均能够取得计算边缘概率所需的一切信息。

考虑归一化的问题，对于一个由有向图转化而来的无向图，由于其概率因子分解的形式，由加和-乘积算法计算的边缘概率分布会类似地被正确归一化，而对于一个无向图，通常会存在一个未知的归一化系数，这需要对单变量的由加和-乘积算法得到的未归一化边缘概率分布进行归一化得到，由于过程在单变量上进行，因此是高效的。

#### 4.2.3 Max-Sum Algorithm

在求出边缘概率分布之后，一个常见的任务是求出使得联合概率分布 $p(\mathbf{x})$ 最大的各变量的取值，寻找这样一组变量组合的任务可以通过最大加和算法高效地完成，其原理与加和-乘积算法有相通之处。

首先我们可以将优化目标写作以下的形式：
$$
\max_\mathbf{x}p(\mathbf{x})=\max_{x_1}\max_{x_2}\dotsb\max_{x_N}p(\mathbf{x})
$$
然后，类似于加和-乘积算法，我们可以考虑对联合概率分布进行因子分解，使得我们可以以类似加和-乘积算法处理加和的的方式求最大值，实际运算中，许多小概率的乘积会导致数值下溢的问题，因此更明智的选择是对联合概率分布的对数进行操作：
$$
\mu_{f\rightarrow x}(x)=\max_{x_1,x_2,\dotsb,x_M}\left[\ln f(x,x_1,x_2,\dotsb,x_M)+\sum_{m\in\mathrm{ne}(f)\backslash x}\mu_{{x_m}\rightarrow f}(x_m)\right]
$$

$$
\mu_{x\rightarrow f}(x)=\sum_{l\in\mathrm{ne}x\backslash f}\mu_{f_l\rightarrow x}(x)
$$

由叶子节点发送的信息是：
$$
\mu_{x\rightarrow f}(x)=0
$$

$$
\mu_{f\rightarrow x}(x)=\ln f(x)
$$

那么，联合概率分布的最大值可以i表示为：
$$
\ln p_{max}=\max_{x}\left[\sum_{s\in\mathrm{ne}(x)}\mu_{f_s\rightarrow x}(x)\right]
$$
现在的问题是如何寻找使联合概率分布达到最大的变量的配置，这可以通过一种不同的由根节点至叶节点的消息传播方式实现。我们可以追踪使前驱变量（靠近根节点的变量）取得最大值的变量（组合）的取值，即记录：
$$
\phi(x)=\arg \max_{x_1,x_2,\dotsb,x_M}\left[\ln f(x,x_1,x_2,\dotsb,x_M)+\mu_{x_m\rightarrow f}(x_m)\right]=x_1,x_2,\dotsb,x_M
$$
对于一个链结构的概率图，可以表述为：
$$
x_{n-1}=\phi(x_n)=\arg \max_{x_{n-1}}[\ln f_{n-1,n}(x_{n-1,x_n})+\mu_{x_{n-1}\rightarrow f_{n-1,n}}(x_{n-1})]
$$
在 Markov chain 中这被称为 Viterbi 算法。

#### 4.2.4 Junction Tree Algorithm

联合树算法是一种对一般图，即包括非树的有环图的精确推断算法，其思路是:

- **伦理化：**对于无向图，首先转化得到其对应的伦理图
- **三角化：**通过向包含四个或更多节点的无弦环中添加额外的边的方式消除无弦环，使得图被三角化
- **识别最大团：**在三角化后的图中找出所有最大团，作为联合树的节点，如果两个团之间存在节点共享，就添加一条边
- **构建联合树：**将团之间的共享变量数作为权重，在最大团的有权无向图中构建最大生成树（如Kruskal算法），确保交集最大化，然后验证树的运行交集性质（RIP），对生成树中的每条路径，检查路径上的所有团是否包含路径两端团的共享变量
- **初始化势函数：**将条件概率分布（CPD）分配到包含对应变量的团中，通过乘法合并为初始势函数
- **校准：**通过两阶段的消息传递使相邻团在共享变量（分离集，Separator）上达成一致

联合树算法是对于任意概率图精确推断代价最低的算法，但是其计算复杂度随最大团大小指数增长，对稠密图不适用。此外，构建联合树的过程本身也相当复杂，增加了计算成本。

#### 4.2.5 Loopy Belief Propagation

循环置信传播算法是一种基于加和-乘积算法的任意图近似推断算法（近似推断算法还包括变分推断、采样推断等），循环置信传播算法直接在原图上进行消息传递，忽略环的存在，通过迭代更新近似计算边际分布。其消息传递机制与树结构概率图的消息传递机制一致，通过迭代式地对消息进行更新，使得运算结果收敛至真实分布附近。

循环置信传播中，更新消息的方式取决于消息传播时间表，在并行时间表中，所有消息同时被更新，而在异步更新时间表中，按固定顺序（如节点遍历顺序或随机顺序）逐个更新消息。时间表的选择直接决定了算法的收敛性。
