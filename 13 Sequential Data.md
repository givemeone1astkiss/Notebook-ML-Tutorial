# Sequential Data

## 1 Markov Model

处理顺序数据的最简单的方式是忽略顺序的性质，将观测看做独立同分布，但是这种处理无法利用数据中的顺序模式。为了在概率模型中表示这种效果，我们需要放松独立同分布的假设，一种最简单的方式是考虑马尔科夫模型，首先，我们可以使用概率的乘积规则来表示观测序列的联合概率分布：
$$
p(x_1,\dotsb,x_N)=p(x_1)\prod_{n=2}^Np(x_n\mid x_1,\dotsb,x_{n-1})
$$
如果我们假设右侧的每个条件概率分布只与最近的一次观测有关，就可以得到一阶的马尔科夫链：
$$
p(x_n\mid x_1,\dotsb,x_{n-1})=p(x_n\mid x_{n-1})\\
p(x_1,\dotsb,x_N)=p(x_1)\prod_{n=1}^Np(x_n\mid x_{n-1})
$$
在这种模型的大部分应用中，条件概率分布 $p(x_n\mid x_{n-1})$ 被限制为相等的，对应于静止时间序列的假设，这类模型被称为同质马尔科夫链（homogeneous Markov chain）。

如果我们允许预测除了与当前观测有关外，还与当前观测的前一次观测有关，那么我们就得到了二阶马尔科夫链：
$$
p(x_1,\dotsb,x_N)=p(x_1)p(x_1\mid x_2)\prod_{n=3}^Np(x_n\mid x_{n-2},x_{n-1})
$$
类似地，我们可以拓展至 $M$ 阶马尔科夫链，但是这种拓展是有代价的：

对于离散变量的情况，假定每个节点的取值有 $K$ 种情况，对于一阶马尔可夫模型，参数量可以表示为 $K(K-1)$，而对于M阶的马尔科模型，参数量表示为 $K!(K-1)$，可见模型的参数量随 $M$ 以指数级增长。

对于连续变量的情况，有两种简化建模的形式：

- 自回归模型（AR model）：基于线性高斯条件概率分布，当前时刻的观测值 $x_n$ 是其过去 *M* 个值的线性组合加高斯噪声：

$$
x_n = c+\sum_{i=1}^M\phi_ix_{n-i}+\epsilon_n,\epsilon_n\sim\mathcal{N}(0,\sigma^2)
$$

- 抽头延迟线模型（tapped delay line）：通过参数化模型（如神经网络）建立条件概率分布 ，利用存储的过去$M$个观测值预测当前值（的均值）：

$$
x_n=f_\theta(x_{n-1},\dotsb,x_{n-M})
$$

假设我们希望构造任意阶数的不受马尔可夫假设限制的序列模型，同时能够使用较少数量的自由参数确定，我们可以引入额外的潜在变量。对于每个观测 $x_n$ 我们引入一个额外的潜在变量 $z_n$，我们现在假设潜在变量构成了马尔科夫链，得到的图结构被称为状态空间模型（state space model）：

![Markov model](./images/13.1..png)

可以看到现在序列 $\{z_n\}$ 具有与一阶马尔科夫链类似的条件独立性，这个模型的联合概率分布为：
$$
p(x_1,\dotsb,x_N,z_1,\dotsb,z_N)=p(z_1)\left[\prod_{n=2}^Np(z_n\mid z_{n-1})\right]\prod_{n=1}^Np(x_n\mid z_n)
$$
对于顺序数据来说，这个图描述了两个重要的模型。如果潜在变量是离散的，那么我们得到 了隐马尔科夫模型（HMM），如果潜在变量和观测变量都是高斯变量（结点的条件概率分布对于父节点的依赖是线性高斯的形式），那么我们就得到了线性动态系统（linear dynamical system）。

## 2 Hidden Markov Models

### 2.1 Introduction to HMM

在 HMM 中，潜在变量是离散的服从多项式分布的 $z_n$，我们使用转移概率 $A$，描述条件概率分布 $p(z_{n}\mid z_{n-1})$，由于潜在变量是离散的，因此转移概率可以使用简单的矩阵表示：$A_{ij}=p(z_{n,j}=1\mid z_{n-1,k}=1)$，因此其满足 $0\leq A_{ij}\leq1$ 且 $\sum_j A_{ij}=1$，因此 $A$ 的独立参数量为 $K(K-1)$，我们可以将条件概率分布写成：
$$
p(z_n\mid z_{n-1},A)=\prod_{i=1}^K\prod_{j=1}^KA_{ij}^{z_{n-1,i},z_{n,j}}
$$
对于初始节点，由于其并没有父节点，因此其边缘概率有一个额外的概率向量 $\pi$ 表示，元素为 $\pi_i=p(z_{1i}=1)$，即：
$$
p(z_1\mid \pi)=\prod_{i=1}^K\pi_{i}^{z_{1i}}
$$
一种比较有用的表示是将状态转移在时间上展开，这就得到了晶格图：

![HMM](./images/13.2.png)

要建立 HMM，还需要定义的量是观测变量的条件概率分布 $p(x_n\mid z_n,\phi)$，其中 $\phi$ 为控制概率分布的参数集合，这些条件概率被称为发射概率，对于连续型的观测变量，$\phi$ 可以表示高斯分布，对于离散型的观测变量，$\phi$ 可以表示条件概率表格。我们可以将发射概率表示为：
$$
p(x_n\mid z_n,\phi)=\prod_{i=1}^Kp(x_n\mid\phi_i)^{z_{ni}}
$$
我们主要关注同质的模型，其中所有控制潜在变量的条件概率分布都共享相同的参数 $A$，所有的发射概率都共享相同的参数 $\phi$ 。从⽽观测变量和潜在变量上的联合概率分布为：
$$
p(X,Z\mid \theta)=p(z_1\mid \pi)\left[\prod_{n=2}^N p(z_n\mid z_{n=1},A)\right]\prod_{m=1}^Np(x_m\mid z_m,\phi),\theta=\{\pi,A,\phi\}
$$
这个标准的 HMM 形式有很多变体，其中之一是从左到右 HMM（left-to-right HMM），它将 $A$ 中 $j<i$ 的元素 $A_{ij}$ 设置为零，此外对于初始状态概率，设置 $p(z_{11})=1$。

隐马尔科夫模型的⼀个强⼤的性质是它对于时间轴上局部的变形（压缩和拉伸）具有某种程度的不变性。

### 2.2 Maximum Likelihood for the HMM

