# Optimization for Training Deep Models

## 1 How Learning Differs from Pure Optimization

用于深度学习训练的优化算法与传统的优化算法的区别是，机器学习通常是间接作用于我们所关注的性能度量 $P$ 的，在学习的过程中，我们只是间接地优化 $P$，希望通过降低代价函数 $J(\theta)$ 来提高 $P$，而纯粹的优化算法直接最小化 $J$ 本身。

童话参观，代价函数可以写作训练集上的期望（平均）的形式：
$$
J(\theta)=\mathbb E_{(x,y)\sim \hat p_{data}}L(f(x;\theta),y)
$$
其中 $L$ 是每个样本的损失函数，$f(x;\theta)$ 是输入 $x$ 时所预测的输出，$\hat p_{data}$ 是经验分布。

通常，我们的目标并不是最小化经验分布上的期望，而是来自数据分布本身的期望：
$$
J^*(\theta)=\mathbb E_{(x,y)\sim p_{data}}L(f(x;\theta),y)
$$

### 1.1 Empirical Risk Minimization (ERM)

机器学习任务中，将 $J^*(\theta)$ 称为风险（risk），最小化风险是机器学习任务的根本目标。然而通常真实的数据分布 $p_{data}$ 对我们是不可见的，一种间接的优化方法是最小化经验风险：
$$
\mathbb E_{x,y\sim \hat p_{data}}[L(f(x;\theta),y)]=\frac1m\sum_{i=1}^mL(f(x^{(i)};\theta),y^{(i)})
$$
其中 $m$ 表示训练样本的数目。基于最小化这种平均训练误差的训练过程被称为经验风险最小化。在此情况下，机器学习仍然和传统的直接优化类似。

然而经验风险最小化很容易造成过拟合因此很多情况下并不可行。最有效的现代优化算法是基于梯度下降的，但是很多有用的损失函数，如 $0-1$ 损失，没有有效的导数。这两个问题说明，在深度学习方法中很少直接使用经验风险最小化作为优化目标。

### 1.2 Surrogate Loss Functions and Early Stopping

有时我们真正关心的损失函数并不能被高效地优化，通常优化代理损失函数作为代替，代理损失函数通常具备一些优点，例如可微、连续等。

某些情况下，使用代理损失函数比原损失函数学到的更多。例如，是应用负对数似然代替阶跃函数作为损失时，在训练集上的 $0-1$ 损失达到 $0$ 之后，在测试集上的 $0-1$ 损失还能持续下降很长时间，这是因为即使 $0-1$ 损失期望是 $0$ 时，我们还能拉开不同类别的距离以改进分类器的鲁棒性，获得一个更强壮的、更值得信赖的分类器，从而相对于简单地最小化训练集上的平均 $0-1$ 损失，它能够从训练数据中抽取更多信息。

一般的优化和我们用于训练算法的优化有一个显著的不同：训练算法通常不会停留在局部极小值点，反之，机器学习通常优化代理损失函数，但是基于提前终止的收敛条件满足时停止，而提前终止使用真实潜在损失函数，如验证集上的 $0-1$ 损失，这导致优化终止时代理函数仍有较大的导数，而纯优化终止时导数较小。

## 1.3 Batch and Minibatch Algorithms

机器学习优化和一般优化算法的另一个区别是，机器学习算法的目标函数通常可以分解为训练样本上的求和。机器学习中的优化算法在计算参数的每一次更新时通常使用整个代价函数中一部分项来估计代价函数的期望值。

最大似然估计问题可以在对数空间分解成各个样本的总和：
$$
\theta_{ML}=\arg\max_{\theta}\sum_{i=1}^m\log p_{model}(x^{(i)},y^{(i)};\theta)
$$
最大化这个总和等价于最大化训练集在经验分布上的期望：
$$
J(\theta)=\mathbb E_{x,y\sim\hat p_{data}}\log p_{model}(x,y;\theta)
$$
优化算法所用到的目标函数 $J$ 中的大多数属性也是训练集上的期望，例如，最常用的属性是梯度：
$$
\nabla_\theta J(\theta)=\mathbb  E_{x,y\sim \hat p_{data}}\nabla_{\theta}\log p_{model}(x,y;\theta)
$$
准确计算这个期望代价极大，因为我们需要首先在每个样本上评估模型，实践中，我们可以从数据集中随机采样少量的样本，然后计算这些样本上的平均值。

采用这种估计方式的另一个动机是，由于 $n$ 个样本均值的方差是 $\sigma/\sqrt{n}$，其中 $\sigma$ 是样本值真实的标准差，分母  $\sqrt n$ 表明使用更多样本估计梯度的方法的回报是低于线性的。

还有一个动机是训练集的冗余，最坏的情况下，训练集中所有的 $m$ 个样本都是彼此相同的拷贝。基于采样的梯度估计可以使用单个样本计算出正确的梯度，而比原来的做法少了 $m$ 倍时间。实践中，我们会发现大量样本对梯度做出的贡献是相似的。

使用整个训练集的优化算法被称为批量或确定性梯度算法，因为它们会在一个大批量中同时处理所有的样本。每次只使用单个样本的优化算法有时被称为随机或者在线算法。大多数深度学习算法介于二者之间，使用一个以上但并非全部的训练样本，传统上这被称为小批量（minibatch）或者小批量随机算法。

这类算法的典型是随机梯度下降，小批量的大小通常由以下几个因素决定：

- 更大的批量会计算更精确的梯度估计，但是回报是小于线性的
- 极小批量通常难以充分利用多核架构。这促使我们使用一些绝对最小批量，低于这个值的小批量不会减少计算时间
- 如果批量处理中的所有样本都可以并行地处理，那么内存消耗和批量大小成正比，对于很多硬件设施，这是批量大小的限制因素
- 在某些硬件上使用特定大小的数组时，运行时间会更少，尤其在使用 GPU 时，通常使用 $2$ 的幂次作为批量大小可以获得更少的运行时间
- 可能是由于小批量在学习过程中加入了噪声，它们会有一些正则化效果。泛化误差通常在批量为 $1$ 时最好，因为梯度估计的高方差，小批量训练需要较小的学习率以保持稳定性。因为降低的学习率和消耗更多的步骤来遍历整个数据集都会产生更多的步骤，所以总的运行时间会非常大。

不同的算法使用不同的方法从小批量中获得不同的信息。有些算法对采样误差比其他算法更加敏感，可能是由于：

- 它们使用了很难在少量样本上精确获得的信息
- 它们以放大采样误差的方式使用信息

仅使用梯度 $g$ 的更新方法相对鲁棒，并且能够使用较小的批量获得成功，使用 Hessian 矩阵的二阶方法通常需要更大的批量。

小批量是随机抽取的这点也很重要。从一组样本中计算出梯度期望的无偏估计要求这些样本是独立的。我们也希望两个连续的梯度估计是互相独立的，因此两个连续的小批量样本也应该是彼此独立的。因此，有必要在抽取之前打乱样本顺序，实践中我们只需要将样本打乱一次，然后按照这个顺序存储起来就够了。之后进行抽取的顺序是固定的，这种偏离真实随机采样的方式并没有大的有害影响。

很多机器学习上的优化方式可以分解为并行地计算呢不同样本上单独的更新，我们在计算小批量 $X$ 上最小化 $J(X)$ 的更新时，同时可以计算其他小批量上的更新。

使用小批量梯度下降的另一个动机是，直到重复使用样本之前，小批量梯度下降都会遵循着真实泛化误差的梯度。很多小批量随机梯度下降方法的实现都会打乱数据顺序依次，然后多次遍历数据来更新参数。第一次遍历时，每个小批量都用来计算真实泛化误差的无偏估计，第二次遍历时，估计将会是有偏的，因为它重新抽取的已经使用过的样本。

在 $x$ 和 $y$ 时离散的时，以上的等价性很容易得到。在这种情况下，泛化误差可以表示为：
$$
J^*(\theta)=\sum_x\sum_yp_{data}(x,y)L(f(x,\theta),y)
$$
上式的准确梯度是：
$$
g=\nabla_\theta J^*(\theta)=J^*(\theta)=\sum_x\sum_yp_{data}(x,y)\nabla_\theta L(f(x,\theta),y)
$$
在变量离散的情况下得到的结果是类似的。因此我们可以从数据生成分布 $p_{data}$ 抽取小批量样本 $\{x^{(1)},\dotsb,x^{(m)}\}$ 以及对应的目标 $y^{(i)}$，然后计算该小批量上损失函数关于 对应参数的梯度：
$$
\hat g=\frac1m\nabla_\theta\sum_iL(f(x^{(i)};\theta),y^{(i)})
$$
因此获得泛化误差准确梯度的无偏估计，最后在泛化误差上使用 SGD 方法在 $\hat g$ 上更新参数。

这个解释只能用于样本没有重复使用的情况，但是对训练数据的额外遍历也会由于减小训练误差而得到足够的好处，以抵消其带来的训练误差和测试误差间差距的增加。

## 2 Challenges in Neural Network Optimization